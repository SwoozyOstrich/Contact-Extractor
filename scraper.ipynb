{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n",
    "from googlesearch import search\n",
    "import time\n",
    "from crochet import setup\n",
    "\n",
    "logging.getLogger('scrapy').propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import burlingame_accounts.csv and covert to list\n",
    "burlingame_accounts = pd.read_csv('data/burlingame_accounts.csv')\n",
    "burlingame_accounts = burlingame_accounts['searchterm'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(tag, n, language):\n",
    "    urls = [url for url in search(query=tag, stop=n, lang=language)][:n]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_urls('beer san francisco', 5 , 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MailSpider(scrapy.Spider):\n",
    "    \n",
    "    name = 'email'\n",
    "    \n",
    "    def parse(self, response):\n",
    "        \n",
    "        links = LxmlLinkExtractor(allow=()).extract_links(response)\n",
    "        links = [str(link.url) for link in links]\n",
    "        links.append(str(response.url))\n",
    "        \n",
    "        for link in links:\n",
    "            yield scrapy.Request(url=link, callback=self.parse_link) \n",
    "            \n",
    "    def parse_link(self, response):\n",
    "        \n",
    "        for word in self.reject:\n",
    "            if word in str(response.url):\n",
    "                return\n",
    "            \n",
    "        html_text = str(response.text)\n",
    "        #mail_list = re.findall('\\w+@\\w+\\.{1}\\w+', html_text)\n",
    "        #mail_list = re.findall('\\w{3,}@\\w{3,}', html_text)\n",
    "        mail_list = re.findall('[a-zA-Z0-9_.+-]{3,}@[a-zA-Z0-9-]{3,}\\.[a-zA-Z0-9-.]{2,}', html_text)\n",
    "        \n",
    "\n",
    "        dic = {'email': mail_list, 'link': str(response.url)}\n",
    "        df = pd.DataFrame(dic)\n",
    "        \n",
    "        df.to_csv(self.path, mode='a', header=False)\n",
    "        df.to_csv(self.path, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_user(question):\n",
    "    response = input(question + ' y/n' + '\\n')\n",
    "    if response == 'y':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def create_file(path):\n",
    "    response = False\n",
    "    if os.path.exists(path):\n",
    "        response = ask_user('File already exists, replace?')\n",
    "        if response == False: return \n",
    "    \n",
    "    with open(path, 'wb') as file: \n",
    "        file.close()\n",
    "\n",
    "def append_file(path):\n",
    "    response = False\n",
    "    if os.path.exists(path):\n",
    "        response = ask_user('File already exists, append?')\n",
    "        if response == False: return \n",
    "    \n",
    "    with open(path, 'ab') as file: \n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup()\n",
    "\n",
    "def get_info(tag, n, language, path, reject=[]):\n",
    "    \n",
    "    #create_file(path)\n",
    "    df = pd.DataFrame(columns=['email', 'link'], index=[0])\n",
    "    df.to_csv(path, mode='a', header=True)\n",
    "    \n",
    "    print('Collecting Google urls...')\n",
    "    google_urls = get_urls(tag, n, language)\n",
    "    \n",
    "    print('Searching for emails...')\n",
    "    #process = CrawlerProcess({'USER_AGENT': 'Mozilla/5.0'})\n",
    "    process = CrawlerRunner({'USER_AGENT': 'Mozilla/5.0'})\n",
    "    process.crawl(MailSpider, start_urls=google_urls, path=path, reject=reject)\n",
    "    #process.start()\n",
    "\n",
    "\n",
    "    print('Cleaning emails...')\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    df.columns = ['email', 'link']\n",
    "    #df = df.loc[:,~df.columns.duplicated()].copy()\n",
    "    df = df.drop_duplicates(subset='email')\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(path, mode='w', header=True)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = ['instagram', 'youtube', 'twitter', 'wiki', 'doordash', 'sentry', 'toasttab', 'yelp', 'restaurantji', 'ssf', 'doordash', '7-eleven', 'order.online', 'bevmo', 'shell', 'wixsite', 'ihg', '1-2-1marketing', 'fairviewevents', 'grubhub', 'smcmvcd', 'communityciviccampus', 'jcplatform', 'bluemagnetinteractive', 'hilton', 'kayak', 'web.archive', 'pubmed', 'sciencedirect', 'exploretock', 'bbb', 'garten', 'tripadvisor', 'getspoonfed', 'rss', 'jotformpro', 'instacart', 'ezcater', 'membersfirst', 'cvs', 'cbayresort', 'bing', 'andolasoft', 'buzzfeednews', 'buzzfeed', 'time', 'washingtonpost', 'eonline', 'warnermedia', 'nbcbayarea', 'bostonglobe', 'foxnews', 'tvguide', 'burlingame', 'speckmediainc', 'cityofpacifica', 'primetimeathleticclub', 'foursquare', 'userway', 'wiley', 'cnn', 'google', 'vitasta', 'worldcat', 'webstop', 'ktvu', 'parksconservancy', 'mystore411', 'creativecommons', 'spothopperapp', 'ethicspoint', 'deadline', 'variety', 'forbes', 'pedropointbrewing', 'sirivisacreative', 'wiley', 'tastecatering', 'spoton', 'socialsharksmarketing', 'yola', 'zendesk']\n",
    "# removed 'facebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = get_info('AMERICAN BULL BAR & GRILL THE', 2, 'pt', 'test.csv', reject=bad_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for account in burlingame_accounts:\n",
    "    df = get_info(account, 1, 'pt', 'burlingame_emails.csv', reject=bad_words)\n",
    "    #time.sleep(2)\n",
    "\n",
    "#df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a9c16435e8e18b963d2b2ed856045e83ee065cb480e373d53542704e1e3736c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
